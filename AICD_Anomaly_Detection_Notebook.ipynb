{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee0132d",
   "metadata": {},
   "source": [
    "# AICD Manufacturing Dataset — Anomaly Detection\n",
    "\n",
    "This notebook covers:\n",
    "1. Data exploration\n",
    "2. Anomaly detection models (Isolation Forest + Autoencoder)\n",
    "3. Model evaluation\n",
    "4. Business interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711a382",
   "metadata": {},
   "source": [
    "### Objective : Build a memory-safe, deployable anomaly detection pipeline for AICD data that flags imminent item-drop risk (and related anomalies) in near real-time, enabling proactive maintenance and reduced downtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38174338",
   "metadata": {},
   "source": [
    "### 1: Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd4f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "TensorFlow import failed: ModuleNotFoundError(\"No module named 'tensorflow.python'\")\n",
      "On Windows, install 'Microsoft Visual C++ Redistributable (x64)' and restart the kernel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\shari\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\fault_injection\\\\fault_injection_service_config_parser.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure TensorFlow is installed in this kernel and importable\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q tensorflow==2.20.0\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "    print(\"TF import OK\")\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow import failed:\", repr(e))\n",
    "    print(\"On Windows, install 'Microsoft Visual C++ Redistributable (x64)' and restart the kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cfbd358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data file: c:\\Users\\shari\\Desktop\\Anomaly_Detection\\authenticIndustrialCloudDataDataset\\data\\Data1.csv\n",
      "TensorFlow available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score\n",
    "\n",
    "# Optional deep learning autoencoder\n",
    "try:\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except Exception:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "# Config (robust paths and sampling to avoid OOM)\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'authenticIndustrialCloudDataDataset', 'data')\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'Data1.csv')\n",
    "CSV_SEP = ';'\n",
    "TARGET_COL = 'Alarm.ItemDroppedError'\n",
    "RANDOM_STATE = 42\n",
    "SAMPLE_ROWS = 300_000  # increased to include anomalies for AP calculation\n",
    "\n",
    "print('Using data file:', DATA_FILE)\n",
    "print('TensorFlow available:', TENSORFLOW_AVAILABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518978aa",
   "metadata": {},
   "source": [
    "### 2: Load & Explore the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d90f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (300710, 96)\n",
      "Columns: 96\n",
      "Missing values: 0\n",
      "Target distribution:\n",
      "Alarm.ItemDroppedError\n",
      "0    298710\n",
      "1      2000\n",
      "Name: count, dtype: int64\n",
      "   Relative time        Date            Time  BellowSoftTouch.SequenceNo  \\\n",
      "0     2508602848  25-09-2019  07:37:22.848,0                         100   \n",
      "1     2669612848  25-09-2019  07:40:03.858,0                         101   \n",
      "2     1550112848  25-09-2019  07:21:24.358,0                         105   \n",
      "\n",
      "   IO.ToolTemperature  IO.CurrentVacuumMotor  Sequence.SoftTouchSequenceNr  \\\n",
      "0                7912                   1382                           100   \n",
      "1                7976                   1177                           101   \n",
      "2                7656                   1081                           105   \n",
      "\n",
      "   Sequence.SoftTouchActuator  Sequence.WantedSoleHeight  \\\n",
      "0                           2                        210   \n",
      "1                           4                       1056   \n",
      "2                           2                        936   \n",
      "\n",
      "   CompletedLog.IdleTime  ...  Alarm.Tool.SoleVacuum  \\\n",
      "0                   8031  ...                      0   \n",
      "1                     16  ...                      0   \n",
      "2                   6103  ...                      0   \n",
      "\n",
      "   Alarm.Tool.SkirtPosition  Alarm.Tool.FilterPressure  \\\n",
      "0                         0                          0   \n",
      "1                         0                          0   \n",
      "2                         0                          0   \n",
      "\n",
      "   Alarm.Tool.BellowPressure  Alarm.Tool.BlowerCabinetTemp  \\\n",
      "0                          0                             0   \n",
      "1                          0                             0   \n",
      "2                          0                             0   \n",
      "\n",
      "   Alarm.Tool.VacuumValveNotON  Alarm.Tool.VacuumValveNotOFF  \\\n",
      "0                            0                             0   \n",
      "1                            0                             0   \n",
      "2                            0                             0   \n",
      "\n",
      "   Alarm.Tool.RakeForceVacuumNotON  Sensor.VacuumVibration  IO.FilterPressure  \n",
      "0                                0                       0              13808  \n",
      "1                                0                       0              13816  \n",
      "2                                0                       0              13856  \n",
      "\n",
      "[3 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_data_balanced(file_path=DATA_FILE, sep=CSV_SEP, total_rows=SAMPLE_ROWS, min_pos=2000, chunksize=50000):\n",
    "    collected_pos = []\n",
    "    collected_neg = []\n",
    "    pos_count = 0\n",
    "    total_count = 0\n",
    "    for chunk in pd.read_csv(file_path, sep=sep, chunksize=chunksize):\n",
    "        chunk = chunk.loc[:, ~chunk.columns.str.contains('^Unnamed')]\n",
    "        if TARGET_COL in chunk.columns:\n",
    "            pos = chunk[chunk[TARGET_COL] == 1]\n",
    "            neg = chunk[chunk[TARGET_COL] == 0]\n",
    "        else:\n",
    "            pos = chunk.iloc[0:0]\n",
    "            neg = chunk\n",
    "        # collect anomalies up to min_pos\n",
    "        if not pos.empty and pos_count < min_pos:\n",
    "            need = min_pos - pos_count\n",
    "            take = min(need, len(pos))\n",
    "            if take > 0:\n",
    "                collected_pos.append(pos.head(take))\n",
    "                pos_count += take\n",
    "                total_count += take\n",
    "        # collect normals up to total_rows\n",
    "        if total_rows is not None and total_count < total_rows and not neg.empty:\n",
    "            remaining = total_rows - total_count\n",
    "            take = min(remaining, len(neg))\n",
    "            if take > 0:\n",
    "                collected_neg.append(neg.sample(n=take, random_state=RANDOM_STATE))\n",
    "                total_count += take\n",
    "        if total_rows is not None and total_count >= total_rows and pos_count >= min_pos:\n",
    "            break\n",
    "    # fallback if no chunks found\n",
    "    if not collected_pos and not collected_neg:\n",
    "        df = pd.read_csv(file_path, sep=sep, nrows=total_rows)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        return df\n",
    "    import numpy as np\n",
    "    parts = []\n",
    "    if collected_pos:\n",
    "        parts.append(pd.concat(collected_pos, ignore_index=True))\n",
    "    if collected_neg:\n",
    "        parts.append(pd.concat(collected_neg, ignore_index=True))\n",
    "    df = pd.concat(parts, ignore_index=True)\n",
    "    if TARGET_COL in df.columns:\n",
    "        # shuffle to mix classes\n",
    "        df = df.sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def eda(df):\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Columns:\", len(df.columns))\n",
    "    print(\"Missing values:\", int(df.isna().sum().sum()))\n",
    "    if TARGET_COL in df.columns:\n",
    "        print(\"Target distribution:\")\n",
    "        print(df[TARGET_COL].value_counts())\n",
    "    print(df.head(3))\n",
    "\n",
    "# Example usage\n",
    "df = load_data_balanced()\n",
    "eda(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7223669",
   "metadata": {},
   "source": [
    "### 3: Preprocessing  \n",
    "### We extract numerical features, handle missing values, and scale them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd9a8f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used (count): 93\n",
      "First 10: ['Relative time', 'BellowSoftTouch.SequenceNo', 'IO.ToolTemperature', 'IO.CurrentVacuumMotor', 'Sequence.SoftTouchSequenceNr', 'Sequence.SoftTouchActuator', 'Sequence.WantedSoleHeight', 'CompletedLog.IdleTime', 'CompletedLog.TimeToPickPos', 'CompletedLog.ToolPickTime']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_for_model(df, scaler=StandardScaler()):\n",
    "    feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if TARGET_COL in feature_cols:\n",
    "        feature_cols.remove(TARGET_COL)\n",
    "    # Guard: if no numeric features, raise a clear message\n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"No numeric features found to model.\")\n",
    "    X = df[feature_cols].fillna(df[feature_cols].mean())\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    return Xs, feature_cols\n",
    "\n",
    "Xs, features = preprocess_for_model(df)\n",
    "print(\"Features used (count):\", len(features))\n",
    "print(\"First 10:\", features[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c37e0",
   "metadata": {},
   "source": [
    "### 4: Train Isolation Forest (Unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e60d4684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination used: 0.019952778424395596\n",
      "Sample anomaly scores: [0.48033607 0.50096356 0.52704616 0.3959773  0.42038036 0.42773469\n",
      " 0.48552228 0.39655881 0.40940552 0.40733634]\n"
     ]
    }
   ],
   "source": [
    "def train_isolation_forest(X_train, contamination=0.01):\n",
    "    iso = IsolationForest(n_estimators=200, contamination=contamination, random_state=RANDOM_STATE)\n",
    "    iso.fit(X_train)\n",
    "    scores = -iso.score_samples(X_train)  # higher = more anomalous\n",
    "    return iso, scores\n",
    "\n",
    "# Estimate contamination from target if available, else default\n",
    "contam = 0.01\n",
    "if TARGET_COL in df.columns:\n",
    "    rate = max(1e-5, float(df[TARGET_COL].mean()))\n",
    "    contam = min(0.02, max(0.001, rate * 3.0))\n",
    "\n",
    "iso, iso_scores = train_isolation_forest(Xs, contamination=contam)\n",
    "print(\"Contamination used:\", contam)\n",
    "print(\"Sample anomaly scores:\", iso_scores[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51060762",
   "metadata": {},
   "source": [
    "### 5: Autoencoder (Optional, Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb81a51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow not available; skipping autoencoder.\n"
     ]
    }
   ],
   "source": [
    "def build_dense_autoencoder(input_dim, encoding_dim=16):\n",
    "    if not TENSORFLOW_AVAILABLE:\n",
    "        raise RuntimeError(\"TensorFlow not available.\")\n",
    "    inp = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(64, activation=\"relu\")(inp)\n",
    "    encoded = layers.Dense(encoding_dim, activation=\"relu\")(x)\n",
    "    decoded = layers.Dense(input_dim, activation=\"linear\")(encoded)\n",
    "    model = keras.Model(inp, decoded)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    autoencoder = build_dense_autoencoder(Xs.shape[1], encoding_dim=16)\n",
    "    autoencoder.summary()\n",
    "else:\n",
    "    print(\"TensorFlow not available; skipping autoencoder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d83b54",
   "metadata": {},
   "source": [
    "### 6: Evaluation (if labels are available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157c9f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score (IsolationForest): 0.06877506515524155\n"
     ]
    }
   ],
   "source": [
    "if TARGET_COL in df.columns:\n",
    "    y_true = df[TARGET_COL].astype(int)\n",
    "    try:\n",
    "        ap = average_precision_score(y_true, iso_scores)\n",
    "        print(\"Average Precision Score (IsolationForest):\", ap)\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute AP:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3db81",
   "metadata": {},
   "source": [
    "### 7: Business Interpretation\n",
    "\n",
    "- **Use-case and outcome:** Detect anomalous sensor behavior to reduce unplanned downtime and scrap, improving OEE and throughput.\n",
    "- **Rarity of events:** If `Alarm.ItemDroppedError` is rare, unsupervised methods (Isolation Forest, optional Autoencoder) help surface early-warning signals.\n",
    "- **Alerting strategy:**\n",
    "  - Define severity tiers from model scores (e.g., Info/Warning/Critical with score thresholds).\n",
    "  - Route Critical alerts to on-call maintenance; batch lower tiers into daily summaries to reduce noise.\n",
    "- **Dashboards and monitoring:**\n",
    "  - Show anomaly rate over time, top contributing sensors/features, and recent incidents.\n",
    "  - Include filtering by machine, shift, product, operator, and recipe.\n",
    "- **Maintenance workflow integration:**\n",
    "  - Create tickets automatically when Critical anomalies persist for N minutes or recur within T hours.\n",
    "  - Attach context windows (pre/post anomaly) and sensor snapshots to work orders.\n",
    "- **Root-cause triage:**\n",
    "  - Display top deviating features per anomaly instance to guide initial inspection.\n",
    "  - Correlate anomalies with changeovers, maintenance logs, and environmental/utility data (e.g., air pressure, temperature).\n",
    "- **KPIs and targets:**\n",
    "  - Track precision@K for alerts, MTTR, avoided downtime, scrap reduction, and alert acknowledgment latency.\n",
    "  - Set acceptance criteria (e.g., precision ≥ 0.6 on Critical alerts, acknowledgment < 10 min).\n",
    "- **False positives/negatives handling:**\n",
    "  - Provide one-click feedback (Valid issue / False alarm) to retrain and recalibrate thresholds.\n",
    "  - Suppress alerts during planned maintenance or known transient states.\n",
    "- **Feedback loop (HITL):**\n",
    "  - Capture maintainer feedback and resolution codes; use them to label data and improve models.\n",
    "  - Periodically review the most frequent false-alarm patterns and adjust features or rules.\n",
    "- **Model monitoring and governance:**\n",
    "  - Monitor data drift (feature distributions), performance drift (AP, precision), and data quality (nulls, ranges).\n",
    "  - Establish retraining cadence (e.g., monthly or after N validated incidents) and change management approvals.\n",
    "- **Explainability and auditability:**\n",
    "  - Log model version, features, thresholds, and explanations per alert for traceability.\n",
    "  - Provide simple reason codes (e.g., “Vacuum pressure variance above baseline”).\n",
    "- **Rollout plan:**\n",
    "  - Start in shadow mode (no-alert) → pilot line with few Critical alerts → plant-wide.\n",
    "  - A/B compare with current rules or thresholds to quantify incremental value.\n",
    "- **SLA and on-call:**\n",
    "  - Define alerting windows, response SLAs, and escalation paths.\n",
    "  - Document an incident playbook for repeated anomaly types.\n",
    "- **Security, privacy, compliance:**\n",
    "  - Control access to anomaly data and explanations; log access for audits.\n",
    "  - Ensure compliance with data retention and regulatory standards.\n",
    "- **Performance and scaling:**\n",
    "  - Specify latency and throughput targets (e.g., scoring within 1s per batch of N rows).\n",
    "  - Use batch scoring for high-volume sensors; stream only KPIs needed for alerting.\n",
    "- **Data enrichment:**\n",
    "  - Join with contextual data (maintenance logs, BOM/recipe, operator rosters, environmental sensors) to improve precision.\n",
    "- **Next steps checklist:**\n",
    "  - Finalize alert thresholds with operations; define SLAs and escalation.\n",
    "  - Build dashboard panels (anomaly trend, top features, recent incidents).\n",
    "  - Instrument feedback capture in CMMS/ticketing and wire back to training data.\n",
    "  - Stand up drift/data-quality monitors and schedule retraining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c36689",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
